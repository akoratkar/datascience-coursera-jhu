We can show that, if the population is infinite, the variance of the sample mean is the population
| variance divided by the sample size. Specifically, Var(X') = sigma^2 / n. Let's work through this
| in four short steps.

...

  |===========================================                                                 |  47%

| Which of the following does Var(X') equal? Here X' represents the sample mean and 'Sum(X_i)'
| represents the sum of the n samples X_1,...X_n. Assume these samples are independent.

1: Var(1/n * Sum(X_i))
2: sigma
3: E(1/n * Sum(X_i))
4: mu

Selection: 1

| You got it right!

  |=============================================                                               |  49%

| Which of the following does Var(1/n * Sum(X_i)') equal?

1: mu/n^2
2: 1/n^2*Var(Sum(X_i))
3: sigma/n
4: 1/n^2*E(Sum(X_i))

Selection: 2

| Keep working like that and you'll get there!

  |===============================================                                             |  51%

| Recall that Var is an expected value and expected values are linear. Also recall that our samples
| X_1, X_2,...,X_n are independent. What does Var(Sum(X_i)) equal?

1: E(Sum(X_i))
2: Var(sigma)
3: E(mu)
4: Sum(Var(X_i))

Selection: 4

| You got it right!

  |=================================================                                           |  53%

| Finally, each X_i comes from a population with variance sigma^2. What does Sum(Var(X_i)) equal? As
| before, Sum is taken over n values.

1: n*(sigma)^2
2: (n^2)*Var(sigma)
3: n*E(mu)
4: n*mu

Selection: 1

| Excellent work!

  |===================================================                                         |  55%

| So we've shown that Var(X')=Var(1/n*Sum(X_i))=(1/n^2)*Var(Sum(X_i))=(1/n^2)*Sum(sigm^2)=sigma^2/n
| for infinite populations when our samples are independent.

qnorm(pnorm(.53)) :53

 Poisson random variables are used to model rates such as the rate of hard drive failures. We write
| X~Poisson(lambda*t) where lambda is the expected count per unit of time and t is the total
| monitoring time.

the Poisson distribution approximates the binomial distribution in certain cases. Recall
| that the binomial distribution is the discrete distribution of the number of successes, k, out of n
| independent binary trials, each with probability p. If n is large and p is small then the Poisson
| distribution with lambda equal to n*p is a good approximation to the binomial distribution.

 Let's dissect this to see what it means. First, 'properly normalized' means that you transformed
| the sample mean X'. You subtracted the population mean mu from it and divided the difference by
| sigma/sqrt(n). Here sigma is the standard deviation of the population and n is the sample size.

Second, the CLT says that for large n, this normalized variable, (X'-mu)/(sigma/sqrt(n)) is almost
| normally distributed with mean 0 and variance 1. Remember that n must be large for the CLT to
| apply.


 Let's rephrase the CLT. Suppose X_1, X_2, ... X_n are independent, identically distributed random
| variables from an infinite population with mean mu and variance sigma^2. Then if n is large, the
| mean of the X's, call it X', is approximately normal with mean mu and variance sigma^2/n. We denote
| this as X'~N(mu,sigma^2/n).

 We know from the CLT that for large n, the sample mean is normal with mean mu and standard
| deviation sigma/sqrt(n).

| Note that for a 95% confidence interval we divide (100-95) by 2 (since we have both left and right
| tails) and add the result to 95 to compute the quantile we need. The 97.5 quantile is actually
| 1.96, but for simplicity it's often just rounded up to 2. If you wanted to find a 90% confidence
| interval what quantile would you want?

1: 85
2: 95
3: 90
4: 100

As we've seen before, in a binomial distribution in which p represents the probability or
| proportion of success, the variance sigma^2 is p(1-p), so the standard error of the sample mean p'
| is sqrt(p(1-p)/n) where n is the sample size. The 95% confidence interval of p is then p' +/-
| 2*sqrt(p(1-p)/n). The 2 in this formula represents what?

| Here's another example of applying this formula from the slides. Suppose you were running for
| office and your pollster polled 100 people. Of these 60 claimed they were going to vote for you.
| You'd like to estimate the true proportion of people who will vote for you and you want to be 95%
| confident of your estimate. We need to find the limits that will contain the true proportion of
| your supporters with 95% confidence, so we'll use the formula p' +/- 1/sqrt(n) to answer this
| question. First, what value would you use for p', the sampled estimate?

1: .56
2: .60
3: 1.00
4: .10

Selection: 2

| Excellent job!

  |=========================================                                                   |  44%

| What would you use for 1/sqrt(n)?

1: 1/10
2: 1/sqrt(56)
3: 1/sqrt(60)
4: 1/100

Selection: 1

| Excellent work!

  |==========================================                                                  |  46%

| The bounds of the interval then are what?

1: .46 and .66
2: .55 and .65
3: I haven't a clue
4: .5 and .7

Est +/- qnorm *std error(Est): qnorm; normal quantilr
Z statistic Z=(X'-mu)/(sigma/sqrt(n))
T statistic t=(X'-mu)/(s/sqrt(n))

Est +/- t-quantile *std error(Est)

As df increases, the t distribution gets more like a standard normal, so it's centered around 0. Also, the t
| assumes that the underlying data are iid Gaussian so the statistic (X' - mu)/(s/sqrt(n)) has n-1 degrees of freedom. df=n-1, n is number of data points (sample size)

| Check the placement of the horizontal now using the R function qt with the arguments .975 for the quantile and 2 for the degrees of freedom (df).

> qt(0.975, 2)
[1] 4.302653

t interval is always wider than the normal. This is because estimating the standard
deviation introduces more uncertainty so a wider interval results.

t-interval is defined as X' +/- t_(n-1)*s/sqrt(n) where t_(n-1) is the relevant quantile.

*Groups with pairing

e.g. mn + c(-1,1)*qt(.975,9)*s/sqrt(10); n=10; 95% internal; 0.975=0.95+0.025

This says that with probability .95 the average difference of effects (between the two drugs) for an
| individual patient is between .7 and 2.46 additional hours of sleep.

Same as > t.test(difference)$conf.int
[1] 0.7001142 2.4598858
attr(,"conf.level")
[1] 0.95

#show 4 different calls to t.test
#display as 4 long array
rbind(
  mn + c(-1, 1) * qt(.975, n-1) * s / sqrt(n),
  as.vector(t.test(difference)$conf.int),
  as.vector(t.test(g2, g1, paired = TRUE)$conf.int),
  as.vector(t.test(extra ~ I(relevel(group, 2)), paired = TRUE, data = sleep)$conf.int)

*Independent Groups with Pooling

(n_x-1)(S_x)^2+(n_y-1)(S_y)^2
ns<-(n_x-1)+(n_y-1)
sp <- sqrt(sp/ns)
sqrt(sum(1/n_x+1/n_y))

132.86-127.44+c(-1,1)*qt(.975,ns)*sp*sqrt(1/8+1/21)

sp <- sqrt((9*var(g1)+9*var(g2))/18): Variance of g1 and g2: 18=10-1+10-1
md+c(-1,1)*qt(.975, 18)*sp*sqrt(1/10+1/10) md=ybar-xbar

Using R function t.test(g2,g1,paired=FALSE,var.equal=TRUE)$conf

*Independent Growups without Pooling
Y'-X' +/- t_df * SE
SE=sqrt((s_1)^2/n_1 + (s_2)^2/n_2)

num <- (15.34^2/8 + 18.23^2/21)^2
den <- 15.34^4/8^2/7 + 18.23^4/21^2/20
132.86-127.44+c(-1,1)*qt(0.975, mydf)*sqrt(15.34^2/8+18.23^2/21)

A Type I error REJECTS a TRUE null hypothesis H_0 and a Type II error ACCEPTS a FALSE null hypothesis H_0.

If you decrease the probability of making
| a Type I error (rejecting a true hypothesis), you increase the probability of making a Type II error (accepting a false one) and vice versa.

If an innocent man is convicted what type of error is this? 1: Type I
Suppose a guilty person is not convicted. What type of error is this? Type II

We choose C so that the probability of a Type I error, alpha, is .05 

This means that alpha, the Type I error rate, is the probability of rejecting the null hypothesis when, in fact, it is correct. We don't want alpha too low because then we would never reject the null hypothesis even if it's false.

SE of a sample mean is s/sqrt(n) where s is SD and n is sample size
Choose C so that the probability that X is greater than C given H_0 is 5%. That is, P(X > C| H_0) is 5%. 
 
3: qnorm(.95) is the value of X for which X>0.05 i.e. 5%

| The 95th percentile of a standard normal distribution is 1.645 standard deviations from the mean, so in our example we have to set C to be 1.645 standard deviations MORE than our hypothesized mean of 30, that is, C = 30 + 1.645 * 1 = 31.645
This means that if our OBSERVED (sample) mean X' >= C, then it's only a 5% chance that a random draw from this N(30,1) distribution is larger than C.
32>31.645 and hence reject H0

Recall the Z score is X'-mu divided by the standard error of the mean.
In this example X'=32, mu=30 and the standard error is 10/sqrt(100)=1.
2>1.645 and hence reject

*** The general rule for rejection is if sqrt(n) * ( X' - mu) / s > Z_{1-alpha}.
statistic is (X'-mu) / s/sqrt(n) which is standard normal.

Suppose our first alternative, H_a, is that mu < mu_0. We would reject H_0 (and accept H_a) when our
observed sample mean is significantly less than mu_0. That is, our test statistic (X'-mu) / s/sqrt(n) is less than Z_alpha. Specifically, it is more than 1.64 standard deviations to the left of the mean mu_0.

| If we accept H_a, that the true mu is greater than the H_0 value mu_0 we would want our test statistic to be greater than 1.64 standard deviations from the mean.

1: at least 1.64 std dev greater than mu_0

H_a: mu_a not equal to mu_0
This means that our
| test statistic (X'-mu) / s/sqrt(n) is less than .025, Z_(alpha/2), or greater than
| .975, Z_(1-alpha/2).
The X values in the
| shaded portions are values which are at least 1.96 standard deviations away from
| the hypothesized mean.

 reject if our test statistic is larger than qt(.975, 15) or smaller than qt(.025,
| 15)

Bottom line here is if you fail to reject the one sided test, you know that you
| will fail to reject the two sided.

T-Statistics 3: the number of estimated std errors between the sample and hypothesized means

Note the 95% confidence interval, 0.8310296 1.1629160, returned by t.test. It does
| not contain the hypothesized population mean 0 so we're pretty confident we can
| safely reject the hypothesis.

if a (1-alpha)% interval contains mu_0, then we fail to reject H_0

 Your comparison tells you how "extreme" the test value is toward the alternative hypothesis. The p-value is the probability under the null hypothesis of obtaining evidence as or more extreme than your test statistic (obtained from your observed data) in the direction of the alternative hypothesis.

So if the p-value (probability of seeing your test statistic) is small, then one of two things happens. EITHER H_0 is true and you have observed a rare event (in this unusual test statistic) OR H_0 is false.

pt(q=2.5, df=15, lower.tail=FALSE): Probability x>q

Quantile associated with 0.95 is qnorm(0.95)=1.644854

p-value is the smallest value of alpha at which you
| will reject the null hypothesis

pnorm(2): P value
[1] 0.9772499: somewhere between .95 (where we rejected) and .99 (where we
| failed to reject). 

 pnorm(2,lower.tail=FALSE) 
[1] 0.02275013: P value

 The general rule is that if the p-value is less than the specified alpha you
| reject the null hypothesis and if it's greater you fail to reject.

> pbinom(7, size=8, prob=.5,
+        lower.tail=TRUE)
[1] 0.9960938

> ppois(9, lambda=5,
+        lower.tail=FALSE)
[1] 0.03182806
Power
alpha: Type 1 error: Reject a true NULL H
beta: Type 2 error: Accept a false NULL H
1-beta=power: Accept a true ALT H

t Statistic: (X'-30)/(s/sqrt(n))
SE: s/sqrt(n)

power is the probability that the true mean mu is greater than the (1-alpha) quantile or qnorm(.95).
It's the area under the blue curve (H_a) to the right of the vertical line.

H_0, X'~ N(mu_0, sigma^2/n) and under H_a, X'~ N(mu_a, sigma^2/n) 

First, power is a function that depends on a specific value of an alternative
| mean, mu_a, which is any value greater than mu_0, the mean hypothesized by H_0.

if mu_a is much bigger than mu_0=30 then the power (probability) is bigger
| than if mu_a is close to 30. As mu_a approaches 30, the mean under H_0, the power
| approaches alpha.
 
Z=(X'-30)/(sigma/sqrt(n))
Ha: X' > Z_95 * (sigma/sqrt(n)) + 30, right?

pnorm, which gives us the probability that a valuedrawn from a normal distribution is greater or less than/equal to a specifiedquantile argument depending on the flag lower.tail.

Power: pnorm with the quantile 30 + Z_95 * (sigma/sqrt(n)) and specify mu_a as our mean argument

30 + Z_95 * (sigma/sqrt(n)) represents the point at which our vertical line falls. It's the point on the null distribution at the (1-alpha) level.

z<-pnorm(0.95)
pnorm(30+z, mean=30, sd=1, lower.tail=FALSE)
[1] 0.05
With the mean set to mu_0 the two distributions, null and alternative, are the same and power=alpha.

pnorm(30+z, mean=32, sd=1, lower.tail=FALSE)
[1] 0.63876
64% as opposed to 5%. When the sample mean is
| quite different from (many standard errors greater than) the mean hypothesized by
| the null hypothesis, the probability of rejecting H_0 when it is false is much
| higher. That is power!

> pnorm(30+z*2, mean=32, sd=2, lower.tail=FALSE)
[1] 0.259511

So keeping the effect size (the ratio delta/sd) constant preserved the power.

 Since power goes up as alpha gets larger would the power of a one-sided test be
| greater or less than the power of the associated two sided test? Greater

Suppose H_a says that mu > mu_0. Then power = 1 - beta = Prob ( X' > mu_0 +
| z_(1-alpha) * sigma/sqrt(n)) assuming that X'~ N(mu_a,sigma^2/n).

Instead only sqrt(n)*(mu_a - mu_0) /sigma is needed. The quantity (mu_a - mu_0) /
| sigma is called the EFFECT SIZE.

P( (X' - mu_0)/(S /sqrt(n)) > t_(1-alpha,
| n-1) 


> power.t.test(n = 16, delta = 2 / 4, sd=1, type = "one.sample", alt = "one.sided")$power
[1] 0.6040329

> power.t.test(n = 16, delta = 2 , sd=4, type = "one.sample", alt = "one.sided")$power 
[1] 0.6040329

> power.t.test(n = 16, delta = 100 , sd=200, type = "one.sample", alt = "one.sided")$power
[1] 0.6040329


> power.t.test(power = .8, delta = 2 / 4, sd=1, type = "one.sample", alt ="one.sided")$n
[1] 26.13751

> power.t.test(power = .8, delta = 2, sd=4, type = "one.sample", alt ="one.sided")$n
[1] 26.13751

> power.t.test(power = .8, n=26, sd=1, type = "one.sample", alt ="one.sided")$delta
[1] 0.5013986

Suppose H_0 proposes mu = mu_0 and H_a proposes that mu < mu_0. Which of the
following is true?

2: the smaller mu_a-mu_0 the more powerful the test
